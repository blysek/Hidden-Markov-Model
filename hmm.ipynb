{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use jurafsky as a guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, states, observations, trans, emis, initial):\n",
    "        '''\n",
    "        states: list of states (even though we are using np.array)\n",
    "        \n",
    "        observations: list of observations\n",
    "        \n",
    "        number_states: number of states (numpy shape: states.shape[0] gives length of array, since its a list)\n",
    "        \n",
    "        number_observations: number of observations (gives length of array, since its a list)\n",
    "        \n",
    "        trans: numpy arrary, matrix of transmission probabilities. Probability of moving from state i (row) to state j (column)\n",
    "        \n",
    "        emis: numpy array, matrix of emission probabilities. Probability of seeing observation o (column) from state s (row)\n",
    "        \n",
    "        initial: numpy array, array of all the starting probabilities. \n",
    "        '''\n",
    "        self.states = np.array(states)\n",
    "        self.observations = np.array(observations)\n",
    "        self.number_states = self.states.shape[0]\n",
    "        self.number_observations = self.observations.shape[0]\n",
    "        self. trans = np.array(trans)\n",
    "        self.emis = np.array(emis)\n",
    "        self.initial = np.array(initial)\n",
    "    \n",
    "    def get_obs(self, o):\n",
    "        #gets the index of the observation\n",
    "       \n",
    "        return np.argwhere(self.observations == o).flatten().item()\n",
    "        #np.argwhere goes through each index and finds where its equal to o\n",
    "        #.flatten() concatenates it into a single array\n",
    "        #.item() turns the argwhere().flatten() to an int\n",
    "    \n",
    "    def forward_alg(self, obs):\n",
    "        #observations: numpy array of observations of length T\n",
    "        \n",
    "        T = len(obs) # len(observations) == np.array(observations).shape[0]\n",
    "        alpha = np.zeros((self.number_states, T)) #creates a number_states x T matrix of zeros \n",
    "        #alphas is forward probability matrix\n",
    "        \n",
    "        #initialize\n",
    "        o_0 = self.get_obs(obs[0]) #gets the index (from list of observation space) of first observation in seq\n",
    "        alpha[:, 0] = initial * emis[:, o_0] #the first column for each row of the forward prob matrix is the initial prob * the emission prob of seeing that obs from the initial state\n",
    "        \n",
    "        #loop through every observation in observation sequence\n",
    "        for t in range(1, T): #we do range(1, T) because we already initialized it\n",
    "            o_t = self.get_obs(obs[t])\n",
    "            alpha[:, t] = alpha[:, t-1].dot(self.trans) * self.emis[:, o_t] #forward algo is prev forward prob (alpha[:, t-1]) * trans prob from previous state i ito current state q\n",
    "        \n",
    "        #sum the probabilities \n",
    "        prob = alpha[:, T-1].sum() # sums the last column column of each row (each row corresponds to a starting state and colums are obs sequence)\n",
    "        \n",
    "        return prob, alpha\n",
    "    \n",
    "    def likelihood(self, obs):\n",
    "        #forward_alg return two things: prob and alpha (matrix), and we may just want the prob of the sequence\n",
    "        prob, alpha = self.forward_alg(obs)\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def backward_alg(self, obs):\n",
    "        #observations: numpy array of observations of length T\n",
    "        T = len(obs)\n",
    "        beta = np.zeros((self.number_states, T))\n",
    "        \n",
    "        #initialize\n",
    "        beta[:, T-1] = 1\n",
    "        \n",
    "        #loop through the observation sequence backwards\n",
    "        for t in range(T-2, -1, -1): #range(start, stop, step)\n",
    "            o_t1 = self.get_obs(obs[t+1])\n",
    "            beta[:, t] = self.trans.dot(self.emis[:, o_t1] * beta[:, t+1])\n",
    "\n",
    "        \n",
    "        o_0 = self.get_obs(obs[0])\n",
    "        prob = self.initial.dot(self.emis[:, o_0] * beta[:, 0])\n",
    "        #prob = (self.initial * beta[:, 0]).dot(self.emis[:, o_0])\n",
    "\n",
    "        return prob, beta\n",
    "    \n",
    "    def viterbi(self, obs):\n",
    "        #viterbi: v_t(j) =max_(1≤i≤N−1)v_t−1(i) * a_ij * b_j(o_t)\n",
    "        #Decoding:  Given as input an HMMλ= (A,B)and a sequence of observations O=o1,o2,...,oT, find the most probable sequence of states Q=q1q2q3...qT.\n",
    "        T = len(obs)\n",
    "        v = np.zeros((self.number_states, T))\n",
    "        prev = np.zeros((T-1, self.number_states))\n",
    "        \n",
    "        #initialize viterbi matrix and observation\n",
    "        o_0 = self.get_obs(obs[0])\n",
    "        v[:, 0] = self.initial * emis[:, o_0]\n",
    "        \n",
    "        #loop through obs and each state\n",
    "        for t in range(1, T):\n",
    "            for n in range(self.number_states):\n",
    "                o_t = self.get_obs(obs[t])\n",
    "                seq_prob = v[:, t-1] * self.trans[:, n] * self.emis[n, o_t]\n",
    "                prev[t-1, n] = np.argmax(seq_prob)\n",
    "                v[n, t] = np.max(seq_prob)\n",
    "                \n",
    "        last_state = int(np.argmax(v[:, T-1]))\n",
    "        path = []\n",
    "        path.append(last_state)\n",
    "        #path_idx = 1\n",
    "        \n",
    "        for t in range(T-2, -1, -1):\n",
    "            path.append(prev[t, last_state])\n",
    "            last_state = int(prev[t, last_state])\n",
    "            #path_idx += 1\n",
    "        \n",
    "        path = list(reversed(path))\n",
    "        prob = v[:, T-1].max()\n",
    "        #path = self.states[v.argmax(axis=0)]\n",
    "        \n",
    "        return path, prob, v, prev\n",
    "    \n",
    "    def baum_welch(self, obs, iterations=1):\n",
    "        #HMM training to learn the trans and emis probabilities given an observation sequence \n",
    "        #aka forward-backward algorithm\n",
    "        \n",
    "        T = len(obs)\n",
    "        \n",
    "        #expectation step (E-step)\n",
    "        f_prob, alpha = self.forward_alg(obs)\n",
    "        b_prob, beta = self.backward_alg(obs)\n",
    "        \n",
    "        #see pg 13 & 14 of https://web.stanford.edu/~jurafsky/slp3/A.pdf\n",
    "        gamma = alpha * beta / (alpha * beta).sum(axis=0)\n",
    "        #xi will have three axes bc xi is the probability of being in state i at time t and state j at time t+1\n",
    "        #we need 3 axes because: 1) we need one for state i. 2) one for state j. 3) one for t\n",
    "        xi = np.zeros((self.number_states, self.number_states, T-1))\n",
    "        \n",
    "        for t in range(T-1):\n",
    "            o_t1 = self.get_obs(obs[t+1])\n",
    "            demon = np.dot(np.dot(alpha[:, t], self.trans) * self.emis[:, o_t1], beta[:, t+1])\n",
    "            for i in range(self.number_states):      \n",
    "                numer = alpha[i, t] * self.trans[i, :] * self.emis[:, o_t1] * beta[:, t+1]\n",
    "                xi[i, :, t] = numer / demon\n",
    "                \n",
    "        #maximization step (M-step)\n",
    "        \n",
    "        self.initial = gamma[:, 0]\n",
    "        self.trans = xi.sum(axis=2) / gamma[:, :-1].sum(axis=1).reshape(-1, 1)\n",
    "        for i, o in enumerate(self.observations):\n",
    "            idx = np.argwhere(obs == o).flatten()\n",
    "            self.emis[:, i] = gamma[:, idx].sum(axis=1) / gamma.sum(axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "update the current emis and trans (so self.emis and self.trans - what im doing now)\n",
    "\n",
    "or\n",
    "\n",
    "create new emis and trans for only buam-welch, use those for calc, and return them as emis and trans prob using\n",
    "just that sequence\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['hot', 'cold'] \n",
    "observation_space = np.array([1, 2, 3])\n",
    "\n",
    "initial = np.array([0.8, 0.2])\n",
    "trans = np.array([[0.6, 0.4], [0.5, 0.5]])\n",
    "emis = [[0.2, 0.4, 0.4], [0.5, 0.4, 0.1]]\n",
    "emis = np.array(emis)\n",
    "hmm = HMM(states, observation_space, trans, emis, initial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.array([1, 2, 3, 2, 2, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path: [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0]\n",
      "\n",
      "Probability: 2.1233664000000008e-05\n",
      "\n",
      "Viterbi matrix: [[1.6000000e-01 3.8400000e-02 9.2160000e-03 2.2118400e-03 5.3084160e-04\n",
      "  6.3700992e-05 2.1233664e-05]\n",
      " [1.0000000e-01 2.5600000e-02 1.5360000e-03 1.4745600e-03 3.5389440e-04\n",
      "  1.0616832e-04 2.1233664e-05]]\n",
      "\n",
      "Prev: [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "path, prob, v, prev = hmm.viterbi(obs)\n",
    "\n",
    "print('Path: {}'.format(path))\n",
    "print()\n",
    "print('Probability: {}'.format(prob))\n",
    "print()\n",
    "print('Viterbi matrix: {}'.format(v))\n",
    "print()\n",
    "print('Prev: {}'.format(prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.16      , 0.0584    , 0.023136  , 0.00647584, 0.00247919,\n",
       "        0.00049362, 0.00031569],\n",
       "       [0.1       , 0.0456    , 0.004616  , 0.00462496, 0.00196113,\n",
       "        0.00098612, 0.0002762 ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path, alpha = hmm.forward_alg(obs)\n",
    "alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baum-Welch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before learning\n",
      "\n",
      "initial:\n",
      " [0.8 0.2]\n",
      "\n",
      "transmission:\n",
      " [[0.6 0.4]\n",
      " [0.5 0.5]]\n",
      "\n",
      "emission:\n",
      " [[0.2 0.4 0.4]\n",
      " [0.5 0.4 0.1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"before learning\")\n",
    "print()\n",
    "print(\"initial:\\n\", hmm.initial)\n",
    "print()\n",
    "print(\"transmission:\\n\", hmm.trans)\n",
    "print()\n",
    "print(\"emission:\\n\", hmm.emis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_obs = np.array([3, 2, 1, 1, 2, 3, 3])\n",
    "hmm.baum_welch(obs, iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after learning\n",
      "\n",
      "learning obs:  [3 2 1 1 2 3 3]\n",
      "\n",
      "initial:\n",
      " [0.65756278 0.34243722]\n",
      "\n",
      "transmission:\n",
      " [[0.51895955 0.48104045]\n",
      " [0.77911806 0.22088194]]\n",
      "\n",
      "emission:\n",
      " [[0.31412435 0.46498682 0.22088882]\n",
      " [0.2394082  0.7449199  0.0156719 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"after learning\")\n",
    "print()\n",
    "print(\"learning obs: \", learn_obs)\n",
    "print()\n",
    "print(\"initial:\\n\", hmm.initial)\n",
    "print()\n",
    "print(\"transmission:\\n\", hmm.trans)\n",
    "print()\n",
    "print(\"emission:\\n\", hmm.emis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
